source ~/miniconda3/etc/profile.d/conda.sh && conda activate medparse-py311 && pytest
........................................................................ [  5%]
........................................................................ [ 10%]
........................................................................ [ 15%]
........................................................................ [ 20%]
........................................................................ [ 25%]
...............sssssssss.................sssssssssssss..........EEEEEEEE [ 30%]
FFFFFFFEEEE............................................................. [ 35%]
........................................................................ [ 40%]
........................................................................ [ 45%]
........................................................................ [ 50%]
........................................................................ [ 55%]
........................................................................ [ 60%]
........................................................................ [ 65%]
........................................................................ [ 70%]
........................................................................ [ 75%]
........................................................................ [ 80%]
........................................................................ [ 85%]
........................................................................ [ 90%]
........................................................................ [ 95%]
......................................................................   [100%]
==================================== ERRORS ====================================
___________ ERROR at setup of TestHealthEndpoints.test_health_check ____________

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

fixturedef = <FixtureDef argname='test_client' scope='function' baseid='tests/ml_advisor'>
request = <SubRequest 'test_client' for <Function test_health_check>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^

../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/pytest_asyncio/plugin.py:728: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/ml_advisor/conftest.py:509: in test_client
    with TestClient(app) as client:
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
---------------------------- Captured stderr setup -----------------------------
{"timestamp": "2026-01-19T00:16:03.888470", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log setup ------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
__________ ERROR at setup of TestHealthEndpoints.test_advisor_status ___________

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

fixturedef = <FixtureDef argname='test_client' scope='function' baseid='tests/ml_advisor'>
request = <SubRequest 'test_client' for <Function test_advisor_status>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^

../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/pytest_asyncio/plugin.py:728: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/ml_advisor/conftest.py:509: in test_client
    with TestClient(app) as client:
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
---------------------------- Captured stderr setup -----------------------------
{"timestamp": "2026-01-19T00:16:04.370701", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log setup ------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
________ ERROR at setup of TestCodeEndpoints.test_code_procedure_basic _________

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

fixturedef = <FixtureDef argname='test_client' scope='function' baseid='tests/ml_advisor'>
request = <SubRequest 'test_client' for <Function test_code_procedure_basic>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^

../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/pytest_asyncio/plugin.py:728: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/ml_advisor/conftest.py:509: in test_client
    with TestClient(app) as client:
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
---------------------------- Captured stderr setup -----------------------------
{"timestamp": "2026-01-19T00:16:04.627463", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log setup ------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
_____ ERROR at setup of TestCodeEndpoints.test_code_procedure_bronchoscopy _____

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

fixturedef = <FixtureDef argname='test_client' scope='function' baseid='tests/ml_advisor'>
request = <SubRequest 'test_client' for <Function test_code_procedure_bronchoscopy>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^

../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/pytest_asyncio/plugin.py:728: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/ml_advisor/conftest.py:509: in test_client
    with TestClient(app) as client:
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
---------------------------- Captured stderr setup -----------------------------
{"timestamp": "2026-01-19T00:16:05.047460", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log setup ------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
____ ERROR at setup of TestCodeEndpoints.test_code_procedure_thoracentesis _____

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

fixturedef = <FixtureDef argname='test_client' scope='function' baseid='tests/ml_advisor'>
request = <SubRequest 'test_client' for <Function test_code_procedure_thoracentesis>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^

../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/pytest_asyncio/plugin.py:728: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/ml_advisor/conftest.py:509: in test_client
    with TestClient(app) as client:
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
---------------------------- Captured stderr setup -----------------------------
{"timestamp": "2026-01-19T00:16:05.307286", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log setup ------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
__________ ERROR at setup of TestCodeEndpoints.test_code_with_advisor __________

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

fixturedef = <FixtureDef argname='test_client' scope='function' baseid='tests/ml_advisor'>
request = <SubRequest 'test_client' for <Function test_code_with_advisor>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^

../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/pytest_asyncio/plugin.py:728: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/ml_advisor/conftest.py:509: in test_client
    with TestClient(app) as client:
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
---------------------------- Captured stderr setup -----------------------------
{"timestamp": "2026-01-19T00:16:05.566233", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log setup ------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
_____ ERROR at setup of TestCodeEndpoints.test_code_with_advisor_disabled ______

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

fixturedef = <FixtureDef argname='test_client' scope='function' baseid='tests/ml_advisor'>
request = <SubRequest 'test_client' for <Function test_code_with_advisor_disabled>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^

../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/pytest_asyncio/plugin.py:728: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/ml_advisor/conftest.py:509: in test_client
    with TestClient(app) as client:
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
---------------------------- Captured stderr setup -----------------------------
{"timestamp": "2026-01-19T00:16:05.975554", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log setup ------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
_ ERROR at setup of TestAdvisorSuggestEndpoint.test_suggest_requires_enabled_advisor _

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

fixturedef = <FixtureDef argname='test_client' scope='function' baseid='tests/ml_advisor'>
request = <SubRequest 'test_client' for <Function test_suggest_requires_enabled_advisor>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^

../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/pytest_asyncio/plugin.py:728: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/ml_advisor/conftest.py:509: in test_client
    with TestClient(app) as client:
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
---------------------------- Captured stderr setup -----------------------------
{"timestamp": "2026-01-19T00:16:06.236231", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log setup ------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
________ ERROR at setup of TestErrorHandling.test_invalid_request_body _________

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

fixturedef = <FixtureDef argname='test_client' scope='function' baseid='tests/ml_advisor'>
request = <SubRequest 'test_client' for <Function test_invalid_request_body>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^

../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/pytest_asyncio/plugin.py:728: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/ml_advisor/conftest.py:509: in test_client
    with TestClient(app) as client:
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
---------------------------- Captured stderr setup -----------------------------
{"timestamp": "2026-01-19T00:16:08.801002", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log setup ------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
_____ ERROR at setup of TestErrorHandling.test_invalid_procedure_category ______

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

fixturedef = <FixtureDef argname='test_client' scope='function' baseid='tests/ml_advisor'>
request = <SubRequest 'test_client' for <Function test_invalid_procedure_category>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^

../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/pytest_asyncio/plugin.py:728: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/ml_advisor/conftest.py:509: in test_client
    with TestClient(app) as client:
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
---------------------------- Captured stderr setup -----------------------------
{"timestamp": "2026-01-19T00:16:09.198439", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log setup ------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
________ ERROR at setup of TestTraceLogging.test_trace_created_on_code _________

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

fixturedef = <FixtureDef argname='test_client' scope='function' baseid='tests/ml_advisor'>
request = <SubRequest 'test_client' for <Function test_trace_created_on_code>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^

../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/pytest_asyncio/plugin.py:728: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/ml_advisor/conftest.py:509: in test_client
    with TestClient(app) as client:
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
---------------------------- Captured stderr setup -----------------------------
{"timestamp": "2026-01-19T00:16:09.451766", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log setup ------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
____________ ERROR at setup of TestTraceLogging.test_trace_disabled ____________

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

fixturedef = <FixtureDef argname='test_client' scope='function' baseid='tests/ml_advisor'>
request = <SubRequest 'test_client' for <Function test_trace_disabled>>

    @pytest.hookimpl(wrapper=True)
    def pytest_fixture_setup(fixturedef: FixtureDef, request) -> object | None:
        asyncio_mode = _get_asyncio_mode(request.config)
        if not _is_asyncio_fixture_function(fixturedef.func):
            if asyncio_mode == Mode.STRICT:
                # Ignore async fixtures without explicit asyncio mark in strict mode
                # This applies to pytest_trio fixtures, for example
>               return (yield)
                        ^^^^^

../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/pytest_asyncio/plugin.py:728: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/ml_advisor/conftest.py:509: in test_client
    with TestClient(app) as client:
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
---------------------------- Captured stderr setup -----------------------------
{"timestamp": "2026-01-19T00:16:09.868795", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log setup ------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
=================================== FAILURES ===================================
_________ TestAdvisorSuggestEndpoint.test_suggest_with_enabled_advisor _________

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

self = <tests.ml_advisor.test_router.TestAdvisorSuggestEndpoint object at 0x17663da10>

    def test_suggest_with_enabled_advisor(self):
        """Suggest endpoint should work when advisor is enabled."""
        from modules.api.fastapi_app import app
    
        # Override the dependency to enable advisor
        def override_config():
            return make_override_config(enabled=True, backend="stub")
    
        app.dependency_overrides[get_advisor_config] = override_config
    
        try:
>           with TestClient(app) as client:

tests/ml_advisor/test_router.py:170: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
----------------------------- Captured stderr call -----------------------------
{"timestamp": "2026-01-19T00:16:06.683407", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log call -------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
__________________ TestTraceEndpoints.test_list_traces_empty ___________________

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

self = <tests.ml_advisor.test_router.TestTraceEndpoints object at 0x17663e390>
tmp_path = PosixPath('/private/var/folders/kf/nr505v1d6sj_5f64rv_831zc0000gn/T/pytest-of-russellmiller/pytest-15/test_list_traces_empty0')

    def test_list_traces_empty(self, tmp_path):
        """Should return empty list when no traces exist."""
        from modules.api.fastapi_app import app
    
        trace_file = tmp_path / "empty_traces.jsonl"
    
        def override_config():
            return make_override_config(trace_path=trace_file)
    
        app.dependency_overrides[get_advisor_config] = override_config
    
        try:
>           with TestClient(app) as client:

tests/ml_advisor/test_router.py:198: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
----------------------------- Captured stderr call -----------------------------
{"timestamp": "2026-01-19T00:16:06.937692", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log call -------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
________________ TestTraceEndpoints.test_list_traces_with_data _________________

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

self = <tests.ml_advisor.test_router.TestTraceEndpoints object at 0x17663ea90>
populated_trace_file = PosixPath('/private/var/folders/kf/nr505v1d6sj_5f64rv_831zc0000gn/T/pytest-of-russellmiller/pytest-15/test_list_traces_with_data0/test_traces.jsonl')

    def test_list_traces_with_data(self, populated_trace_file):
        """Should return traces when file has data."""
        from modules.api.fastapi_app import app
    
        def override_config():
            return make_override_config(trace_path=populated_trace_file)
    
        app.dependency_overrides[get_advisor_config] = override_config
    
        try:
>           with TestClient(app) as client:

tests/ml_advisor/test_router.py:217: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
----------------------------- Captured stderr call -----------------------------
{"timestamp": "2026-01-19T00:16:07.191839", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log call -------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
________________ TestTraceEndpoints.test_list_traces_pagination ________________

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

self = <tests.ml_advisor.test_router.TestTraceEndpoints object at 0x17663f1d0>
populated_trace_file = PosixPath('/private/var/folders/kf/nr505v1d6sj_5f64rv_831zc0000gn/T/pytest-of-russellmiller/pytest-15/test_list_traces_pagination0/test_traces.jsonl')

    def test_list_traces_pagination(self, populated_trace_file):
        """Should support pagination."""
        from modules.api.fastapi_app import app
    
        def override_config():
            return make_override_config(trace_path=populated_trace_file)
    
        app.dependency_overrides[get_advisor_config] = override_config
    
        try:
>           with TestClient(app) as client:

tests/ml_advisor/test_router.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
----------------------------- Captured stderr call -----------------------------
{"timestamp": "2026-01-19T00:16:07.640553", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log call -------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
_________________ TestTraceEndpoints.test_get_trace_not_found __________________

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

self = <tests.ml_advisor.test_router.TestTraceEndpoints object at 0x17663f990>
tmp_path = PosixPath('/private/var/folders/kf/nr505v1d6sj_5f64rv_831zc0000gn/T/pytest-of-russellmiller/pytest-15/test_get_trace_not_found0')

    def test_get_trace_not_found(self, tmp_path):
        """Should return 404 for non-existent trace."""
        from modules.api.fastapi_app import app
    
        trace_file = tmp_path / "traces.jsonl"
        trace_file.touch()
    
        def override_config():
            return make_override_config(trace_path=trace_file)
    
        app.dependency_overrides[get_advisor_config] = override_config
    
        try:
>           with TestClient(app) as client:

tests/ml_advisor/test_router.py:259: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
----------------------------- Captured stderr call -----------------------------
{"timestamp": "2026-01-19T00:16:07.877870", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log call -------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
____________________ TestMetricsEndpoint.test_metrics_empty ____________________

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

self = <tests.ml_advisor.test_router.TestMetricsEndpoint object at 0x176650210>
tmp_path = PosixPath('/private/var/folders/kf/nr505v1d6sj_5f64rv_831zc0000gn/T/pytest-of-russellmiller/pytest-15/test_metrics_empty0')

    def test_metrics_empty(self, tmp_path):
        """Should return empty metrics when no traces exist."""
        from modules.api.fastapi_app import app
    
        trace_file = tmp_path / "empty_traces.jsonl"
    
        def override_config():
            return make_override_config(trace_path=trace_file)
    
        app.dependency_overrides[get_advisor_config] = override_config
    
        try:
>           with TestClient(app) as client:

tests/ml_advisor/test_router.py:281: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
----------------------------- Captured stderr call -----------------------------
{"timestamp": "2026-01-19T00:16:08.299558", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log call -------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
__________________ TestMetricsEndpoint.test_metrics_with_data __________________

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
>           runtime_warnings = verify_registry_runtime_bundle()
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

modules/api/fastapi_app.py:236: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def verify_registry_runtime_bundle(
        *,
        backend: str | None = None,
        runtime_dir: Path | None = None,
    ) -> list[str]:
        """Validate registry model artifacts for the configured backend.
    
        Returns a list of warnings. Raises RuntimeError if required artifacts are missing.
        """
        resolved_backend = (backend or resolve_model_backend()).strip().lower()
        runtime_dir = runtime_dir or get_registry_runtime_dir()
    
        warnings: list[str] = []
        errors: list[str] = []
    
        def _first_existing(paths: list[Path]) -> Path | None:
            for path in paths:
                if path.exists():
                    return path
            return None
    
        def _require(path: Path, label: str) -> None:
            if not path.exists():
                errors.append(f"Missing {label} at {path}")
    
        def _check_pytorch() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            config_path = runtime_dir / "config.json"
            tokenizer_dir = runtime_dir / "tokenizer"
            thresholds_path = runtime_dir / "thresholds.json"
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
            weights_path = _first_existing(
                [runtime_dir / "model.safetensors", runtime_dir / "pytorch_model.bin"]
            )
            classifier_path = runtime_dir / "classifier.pt"
    
            for path, label in [
                (config_path, "config.json"),
                (tokenizer_dir, "tokenizer/"),
                (thresholds_path, "thresholds.json"),
                (classifier_path, "classifier.pt"),
            ]:
                if not path.exists():
                    local_errors.append(f"Missing {label} at {path}")
    
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            if weights_path is None:
                local_errors.append(
                    "Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        def _check_onnx() -> tuple[list[str], list[str]]:
            local_errors: list[str] = []
            local_warnings: list[str] = []
    
            model_path = _first_existing(
                [
                    runtime_dir / "registry_model_int8.onnx",
                    runtime_dir / "registry_model.onnx",
                ]
            )
            tokenizer_dir = _first_existing(
                [runtime_dir / "tokenizer", runtime_dir / "roberta_registry_tokenizer"]
            )
            thresholds_path = _first_existing(
                [
                    runtime_dir / "thresholds.json",
                    runtime_dir / "registry_thresholds.json",
                    runtime_dir / "roberta_registry_thresholds.json",
                ]
            )
            label_path = _first_existing(
                [runtime_dir / "label_order.json", runtime_dir / "registry_label_fields.json"]
            )
    
            if model_path is None:
                local_errors.append("Missing ONNX model (registry_model_int8.onnx or registry_model.onnx)")
            else:
                data_path = model_path.with_suffix(model_path.suffix + ".data")
                if not data_path.exists():
                    try:
                        size_bytes = model_path.stat().st_size
                    except OSError:
                        size_bytes = 0
                    if size_bytes < 50_000_000:
                        local_errors.append(
                            f"Missing ONNX external data file at {data_path}"
                        )
                    else:
                        local_warnings.append(
                            f"ONNX external data file not found at {data_path}; "
                            "assuming weights are embedded."
                        )
    
            if tokenizer_dir is None:
                local_errors.append("Missing tokenizer directory in runtime bundle")
            if thresholds_path is None:
                local_errors.append("Missing thresholds.json in runtime bundle")
            if label_path is None:
                local_errors.append(
                    "Missing label_order.json or registry_label_fields.json in runtime bundle"
                )
    
            return local_errors, local_warnings
    
        if resolved_backend == "auto":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            warnings.extend(pytorch_warnings)
            if not pytorch_errors:
                return warnings
            onnx_errors, onnx_warnings = _check_onnx()
            warnings.extend(onnx_warnings)
            if not onnx_errors:
                return warnings
            errors.append("No usable registry runtime bundle found for backend=auto")
            errors.extend([f"pytorch: {msg}" for msg in pytorch_errors])
            errors.extend([f"onnx: {msg}" for msg in onnx_errors])
        elif resolved_backend == "pytorch":
            pytorch_errors, pytorch_warnings = _check_pytorch()
            errors.extend(pytorch_errors)
            warnings.extend(pytorch_warnings)
        elif resolved_backend == "onnx":
            onnx_errors, onnx_warnings = _check_onnx()
            errors.extend(onnx_errors)
            warnings.extend(onnx_warnings)
        else:
            errors.append(f"Unknown backend '{resolved_backend}' for registry runtime validation")
    
        if errors:
>           raise RuntimeError("; ".join(errors))
E           RuntimeError: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/registry/model_runtime.py:209: RuntimeError

The above exception was the direct cause of the following exception:

self = <tests.ml_advisor.test_router.TestMetricsEndpoint object at 0x176650910>
populated_trace_file = PosixPath('/private/var/folders/kf/nr505v1d6sj_5f64rv_831zc0000gn/T/pytest-of-russellmiller/pytest-15/test_metrics_with_data0/test_traces.jsonl')

    def test_metrics_with_data(self, populated_trace_file):
        """Should calculate metrics from traces."""
        from modules.api.fastapi_app import app
    
        def override_config():
            return make_override_config(trace_path=populated_trace_file)
    
        app.dependency_overrides[get_advisor_config] = override_config
    
        try:
>           with TestClient(app) as client:

tests/ml_advisor/test_router.py:299: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:688: in __enter__
    portal.call(self.wait_startup)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:321: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:723: in wait_startup
    await receive()
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:714: in receive
    self.task.result()
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:449: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/concurrent/futures/_base.py:401: in __get_result
    raise self._exception
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/anyio/from_thread.py:252: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/testclient.py:704: in lifespan
    await self.app(scope, self.stream_receive.receive, self.stream_send.send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/applications.py:1134: in __call__
    await super().__call__(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/errors.py:151: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/base.py:103: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/cors.py:77: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/middleware/exceptions.py:49: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:725: in app
    await self.lifespan(scope, receive, send)
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/starlette/routing.py:694: in lifespan
    async with self.lifespan_context(app) as maybe_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
../../miniconda3/envs/medparse-py311/lib/python3.11/site-packages/fastapi/routing.py:211: in merged_lifespan
    async with original_context(app) as maybe_original_state:
../../miniconda3/envs/medparse-py311/lib/python3.11/contextlib.py:210: in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

app = <fastapi.applications.FastAPI object at 0x110e56f50>

    @asynccontextmanager
    async def lifespan(app: FastAPI) -> AsyncIterator[None]:
        """Application lifespan with resource management.
    
        This replaces the deprecated @app.on_event("startup") pattern.
    
        Startup:
        - Initializes readiness state (/health vs /ready)
        - Starts heavy model warmup (background by default)
    
        Shutdown:
        - Placeholder for cleanup if needed in the future
    
        Environment variables (see modules.infra.settings.InfraSettings):
        - SKIP_WARMUP / PROCSUITE_SKIP_WARMUP: Skip warmup entirely
        - BACKGROUND_WARMUP: Run warmup in the background (default: true)
        - WAIT_FOR_READY_S: Optional await time for readiness gating
        """
        # Import here to avoid circular import at module load time
        from modules.infra.nlp_warmup import (
            should_skip_warmup as _should_skip_warmup,
        )
        from modules.infra.nlp_warmup import (
            warm_heavy_resources_sync as _warm_heavy_resources_sync,
        )
        from modules.infra.settings import get_infra_settings
    
        _validate_startup_env()
    
        settings = get_infra_settings()
        logger = logging.getLogger(__name__)
        from modules.registry.model_runtime import verify_registry_runtime_bundle
    
        # Readiness state (liveness vs readiness)
        app.state.model_ready = False
        app.state.model_error = None
        app.state.ready_event = asyncio.Event()
        app.state.cpu_executor = ThreadPoolExecutor(max_workers=settings.cpu_workers)
        app.state.llm_sem = asyncio.Semaphore(settings.llm_concurrency)
        app.state.llm_http = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=10.0,
                read=float(settings.llm_timeout_s),
                write=30.0,
                pool=30.0,
            )
        )
    
        # Ensure PHI database tables exist (auto-create on startup)
        try:
            from modules.api.phi_dependencies import engine as phi_engine
            from modules.phi import models as _phi_models  # noqa: F401 - register models
            from modules.phi.db import Base as PHIBase
    
            PHIBase.metadata.create_all(bind=phi_engine)
            logger.info("PHI database tables verified/created")
        except Exception as e:
            logger.warning(f"Could not initialize PHI tables: {e}")
    
        try:
            runtime_warnings = verify_registry_runtime_bundle()
            for warning in runtime_warnings:
                logger.warning("Registry runtime bundle warning: %s", warning)
        except RuntimeError as exc:
>           raise RuntimeError(f"Registry runtime bundle validation failed: {exc}") from exc
E           RuntimeError: Registry runtime bundle validation failed: Missing config.json at data/models/registry_runtime/config.json; Missing classifier.pt at data/models/registry_runtime/classifier.pt; Missing model weights (model.safetensors or pytorch_model.bin) in runtime bundle

modules/api/fastapi_app.py:240: RuntimeError
----------------------------- Captured stderr call -----------------------------
{"timestamp": "2026-01-19T00:16:08.549381", "level": "INFO", "logger": "modules.api.fastapi_app", "message": "PHI database tables verified/created"}
------------------------------ Captured log call -------------------------------
INFO     modules.api.fastapi_app:fastapi_app.py:231 PHI database tables verified/created
=============================== warnings summary ===============================
tests/api/test_fastapi.py::test_coder_run_fixture_response
tests/phi/test_presidio_nlp_backend_smoke.py::test_presidio_scrubber_initializes_with_backend[scispacy-model_candidates1]
  /Users/russellmiller/miniconda3/envs/medparse-py311/lib/python3.11/site-packages/spacy/language.py:2195: FutureWarning: Possible set union at position 6328
    deserializers["tokenizer"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
SKIPPED [9] tests/integration/api/test_startup_warmup.py: _do_heavy_warmup was removed; warmup tests need to be updated
SKIPPED [1] tests/integration/persistence/test_supabase_procedure_store.py:99: Supabase tests disabled (set RUN_SUPABASE_TESTS=1 to enable)
SKIPPED [1] tests/integration/persistence/test_supabase_procedure_store.py:107: Supabase tests disabled (set RUN_SUPABASE_TESTS=1 to enable)
SKIPPED [1] tests/integration/persistence/test_supabase_procedure_store.py:111: Supabase tests disabled (set RUN_SUPABASE_TESTS=1 to enable)
SKIPPED [1] tests/integration/persistence/test_supabase_procedure_store.py:127: Supabase tests disabled (set RUN_SUPABASE_TESTS=1 to enable)
SKIPPED [1] tests/integration/persistence/test_supabase_procedure_store.py:162: Supabase tests disabled (set RUN_SUPABASE_TESTS=1 to enable)
SKIPPED [1] tests/integration/persistence/test_supabase_procedure_store.py:185: Supabase tests disabled (set RUN_SUPABASE_TESTS=1 to enable)
SKIPPED [1] tests/integration/persistence/test_supabase_procedure_store.py:216: Supabase tests disabled (set RUN_SUPABASE_TESTS=1 to enable)
SKIPPED [1] tests/integration/persistence/test_supabase_procedure_store.py:244: Supabase tests disabled (set RUN_SUPABASE_TESTS=1 to enable)
SKIPPED [1] tests/integration/persistence/test_supabase_procedure_store.py:258: Supabase tests disabled (set RUN_SUPABASE_TESTS=1 to enable)
SKIPPED [1] tests/integration/persistence/test_supabase_procedure_store.py:298: Supabase tests disabled (set RUN_SUPABASE_TESTS=1 to enable)
SKIPPED [1] tests/integration/persistence/test_supabase_procedure_store.py:335: Supabase tests disabled (set RUN_SUPABASE_TESTS=1 to enable)
SKIPPED [1] tests/integration/persistence/test_supabase_procedure_store.py:363: Supabase tests disabled (set RUN_SUPABASE_TESTS=1 to enable)
SKIPPED [1] tests/integration/persistence/test_supabase_procedure_store.py:390: Supabase tests disabled (set RUN_SUPABASE_TESTS=1 to enable)
ERROR tests/ml_advisor/test_router.py::TestHealthEndpoints::test_health_check
ERROR tests/ml_advisor/test_router.py::TestHealthEndpoints::test_advisor_status
ERROR tests/ml_advisor/test_router.py::TestCodeEndpoints::test_code_procedure_basic
ERROR tests/ml_advisor/test_router.py::TestCodeEndpoints::test_code_procedure_bronchoscopy
ERROR tests/ml_advisor/test_router.py::TestCodeEndpoints::test_code_procedure_thoracentesis
ERROR tests/ml_advisor/test_router.py::TestCodeEndpoints::test_code_with_advisor
ERROR tests/ml_advisor/test_router.py::TestCodeEndpoints::test_code_with_advisor_disabled
ERROR tests/ml_advisor/test_router.py::TestAdvisorSuggestEndpoint::test_suggest_requires_enabled_advisor
ERROR tests/ml_advisor/test_router.py::TestErrorHandling::test_invalid_request_body
ERROR tests/ml_advisor/test_router.py::TestErrorHandling::test_invalid_procedure_category
ERROR tests/ml_advisor/test_router.py::TestTraceLogging::test_trace_created_on_code
ERROR tests/ml_advisor/test_router.py::TestTraceLogging::test_trace_disabled
FAILED tests/ml_advisor/test_router.py::TestAdvisorSuggestEndpoint::test_suggest_with_enabled_advisor
FAILED tests/ml_advisor/test_router.py::TestTraceEndpoints::test_list_traces_empty
FAILED tests/ml_advisor/test_router.py::TestTraceEndpoints::test_list_traces_with_data
FAILED tests/ml_advisor/test_router.py::TestTraceEndpoints::test_list_traces_pagination
FAILED tests/ml_advisor/test_router.py::TestTraceEndpoints::test_get_trace_not_found
FAILED tests/ml_advisor/test_router.py::TestMetricsEndpoint::test_metrics_empty
FAILED tests/ml_advisor/test_router.py::TestMetricsEndpoint::test_metrics_with_data
7 failed, 1397 passed, 22 skipped, 2 warnings, 12 errors in 32.07s
make: *** [test] Error 1
