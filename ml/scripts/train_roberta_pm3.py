#!/usr/bin/env python3
"""Train RoBERTa-PM-M3-Voc-distill for multi-label registry procedure classification.

This script trains the local RoBERTa-PM-M3-Voc-distill model for predicting registry
procedure flags from clinical procedure notes.

Pipeline Context:
    - Input: data/ml_training/registry_train.csv (Generated by data_prep.py)
    - The input CSV is expected to contain pre-scrubbed text if PHI redaction was
      performed during the data preparation phase.

Key Features:
    - Head + Tail truncation (First 382 + Last 128 tokens)
    - pos_weight calculation for handling severe class imbalance
    - Per-class threshold optimization on validation set
    - Mixed precision training (fp16)

Usage:
    python ml/scripts/train_roberta_pm3.py
    python ml/scripts/train_roberta_pm3.py --batch-size 16 --epochs 8
    python ml/scripts/train_roberta_pm3.py --evaluate-only --model-dir data/models/roberta_pm3_registry
"""

from __future__ import annotations

import argparse
import json
import shutil
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.metrics import f1_score, precision_recall_fscore_support
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from transformers import (
    AutoConfig,
    AutoModel,
    AutoTokenizer,
    get_linear_schedule_with_warmup,
)

from ml.lib.ml_coder.distillation_io import load_label_fields_json
from ml.lib.ml_coder.registry_label_schema import DORMANT_LABELS, REGISTRY_LABELS

# ============================================================================
# Configuration
# ============================================================================

@dataclass
class TrainingConfig:
    """Configuration for RoBERTa-PM-M3-Voc-distill training."""

    # Model
    model_name: str = "data/models/RoBERTa-base-PM-M3-Voc-distill/RoBERTa-base-PM-M3-Voc-distill-hf"

    # Data paths (aligned with ml/lib/ml_coder/data_prep.py)
    train_csv: Path = field(default_factory=lambda: Path("data/ml_training/registry_train.csv"))
    val_csv: Path = field(default_factory=lambda: Path("data/ml_training/registry_val.csv"))
    test_csv: Path = field(default_factory=lambda: Path("data/ml_training/registry_test.csv"))
    label_fields_json: Path = field(default_factory=lambda: Path("data/ml_training/registry_label_fields.json"))
    output_dir: Path = field(default_factory=lambda: Path("data/models/roberta_pm3_registry"))

    # Tokenization
    max_length: int = 512
    head_tokens: int = 382  # First N tokens to keep
    tail_tokens: int = 128  # Last N tokens to keep

    # Training hyperparameters
    batch_size: int = 16
    learning_rate: float = 2e-5
    num_epochs: int = 5
    warmup_ratio: float = 0.1
    weight_decay: float = 0.01
    gradient_accumulation_steps: int = 1
    max_grad_norm: float = 1.0

    # Hardware
    fp16: bool = True
    device: str = field(default_factory=lambda: "cuda" if torch.cuda.is_available() else "cpu")

    # Class imbalance
    pos_weight_cap: float = 100.0

    # Validation
    val_split: float = 0.1

    # Dormant labels (exclude from ML head)
    dormant_mode: str = "auto"  # "auto" | "schema" | "none"
    dormant_min_positives: int = 20

    # Logging
    logging_steps: int = 50


# ============================================================================
# Data Loading
# ============================================================================

def load_label_fields(path: Path) -> list[str]:
    """Load canonical label ordering from JSON."""
    if not path.exists():
        return []
    with open(path, "r") as f:
        return json.load(f)

def load_registry_csv(path: Path, required_labels: list[str] = None) -> tuple[list[str], np.ndarray, list[str]]:
    """Load registry training/test CSV file.

    Args:
        path: Path to CSV file
        required_labels: List of label columns to enforce order.

    Returns:
        (texts, labels_matrix, label_names)
    """
    if not path.exists():
        raise FileNotFoundError(f"CSV file not found: {path}")

    df = pd.read_csv(path)
    if "note_text" not in df.columns:
        raise ValueError(f"File {path} missing 'note_text' column.")

    texts = df["note_text"].fillna("").astype(str).tolist()

    if required_labels:
        # Enforce specific schema (crucial for inference consistency)
        label_cols = required_labels
        for col in label_cols:
            if col not in df.columns:
                # Add missing column as 0 (e.g. rare label present in train but not test)
                df[col] = 0
    else:
        # Infer labels from binary columns
        label_cols = []
        for col in df.columns:
            if col == "note_text":
                continue
            # Simple heuristic: if column is numeric, assume it's a label
            if pd.api.types.is_numeric_dtype(df[col]):
                label_cols.append(col)
        label_cols.sort()

    y = df[label_cols].fillna(0).astype(int).to_numpy()
    
    # Clip to [0, 1] just in case
    y = np.clip(y, 0, 1)

    print(f"Loaded {len(texts)} samples from {path}")
    return texts, y, label_cols


class HeadTailTokenizer:
    """Tokenizer keeping the start (procedure) and end (complications/plan) of notes."""
    def __init__(self, tokenizer, max_length=512, head_tokens=382, tail_tokens=128):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.head_tokens = head_tokens
        self.tail_tokens = tail_tokens

    def __call__(self, text: str):
        tokens = self.tokenizer(text, add_special_tokens=False, truncation=False, return_tensors="pt")
        input_ids = tokens["input_ids"][0]

        if len(input_ids) > (self.max_length - 2):
            head = input_ids[:self.head_tokens]
            tail = input_ids[-self.tail_tokens:]
            input_ids = torch.cat([head, tail])

        # Add [CLS] and [SEP]
        input_ids = torch.cat([
            torch.tensor([self.tokenizer.cls_token_id]),
            input_ids,
            torch.tensor([self.tokenizer.sep_token_id])
        ])

        # Pad
        pad_len = self.max_length - len(input_ids)
        if pad_len > 0:
            input_ids = torch.cat([input_ids, torch.full((pad_len,), self.tokenizer.pad_token_id)])

        mask = (input_ids != self.tokenizer.pad_token_id).long()
        return {"input_ids": input_ids.long(), "attention_mask": mask}


class RegistryDataset(Dataset):
    def __init__(self, texts, labels, tokenizer):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        data = self.tokenizer(self.texts[idx])
        data["labels"] = torch.tensor(self.labels[idx], dtype=torch.float32)
        return data


# ============================================================================
# Model & Training
# ============================================================================

class RoBERTaPM3MultiLabel(nn.Module):
    def __init__(self, model_name, num_labels, pos_weight=None):
        super().__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
        self.register_buffer("pos_weight", pos_weight)

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled = self.dropout(outputs.last_hidden_state[:, 0, :])
        logits = self.classifier(pooled)
        
        loss = None
        if labels is not None:
            loss_fn = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)
            loss = loss_fn(logits, labels)
            
        return {"logits": logits, "loss": loss}

    def save_pretrained(self, path: Path):
        path.mkdir(parents=True, exist_ok=True)
        self.bert.save_pretrained(path)
        torch.save(self.classifier.state_dict(), path / "classifier.pt")
        if self.pos_weight is not None:
            torch.save(self.pos_weight, path / "pos_weight.pt")

    @classmethod
    def from_pretrained(cls, path: Path, num_labels: int):
        path = Path(path)
        pos_weight_path = path / "pos_weight.pt"
        pos_weight = torch.load(pos_weight_path, map_location="cpu") if pos_weight_path.exists() else None
        model = cls(str(path), num_labels, pos_weight=pos_weight)
        classifier_path = path / "classifier.pt"
        if classifier_path.exists():
            model.classifier.load_state_dict(torch.load(classifier_path, map_location="cpu"))
        return model


def _select_active_labels(schema_labels: list[str], train_y: np.ndarray, config: TrainingConfig) -> tuple[list[str], list[str]]:
    dormant: set[str]
    if config.dormant_mode == "schema":
        dormant = set(DORMANT_LABELS)
    elif config.dormant_mode == "auto":
        dormant = set(DORMANT_LABELS)
        min_pos = max(0, int(config.dormant_min_positives))
        counts = train_y.sum(axis=0).astype(int)
        for label, count in zip(schema_labels, counts.tolist()):
            if count < min_pos:
                dormant.add(label)
    elif config.dormant_mode == "none":
        dormant = set()
    else:
        raise ValueError(f"Unknown dormant_mode: {config.dormant_mode}")

    active = [l for l in schema_labels if l not in dormant]
    dormant_sorted = [l for l in schema_labels if l in dormant]
    return active, dormant_sorted


def _thresholds_05(labels: list[str]) -> dict[str, float]:
    return {l: 0.5 for l in labels}


def _metrics_at_thresholds(
    y_true: np.ndarray,
    probs: np.ndarray,
    labels: list[str],
    thresholds: dict[str, float],
) -> dict[str, Any]:
    preds = np.zeros_like(probs, dtype=int)
    for i, label in enumerate(labels):
        preds[:, i] = (probs[:, i] >= float(thresholds.get(label, 0.5))).astype(int)

    precision, recall, f1, _ = precision_recall_fscore_support(y_true, preds, average=None, zero_division=0)
    return {
        "macro_f1": float(np.mean(f1)),
        "micro_f1": float(f1_score(y_true.ravel(), preds.ravel(), zero_division=0)),
        "per_label": {
            label: {
                "precision": float(precision[i]),
                "recall": float(recall[i]),
                "f1": float(f1[i]),
                "support": int(y_true[:, i].sum()),
                "threshold": float(thresholds.get(label, 0.5)),
            }
            for i, label in enumerate(labels)
        },
    }


def train(config: TrainingConfig) -> dict[str, Any]:
    print("--- Training RoBERTa-PM3 Registry Model ---")

    schema_labels = list(REGISTRY_LABELS)
    config.label_fields_json.parent.mkdir(parents=True, exist_ok=True)
    config.label_fields_json.write_text(json.dumps(schema_labels, indent=2) + "\n", encoding="utf-8")

    train_texts, train_y_full, _ = load_registry_csv(config.train_csv, required_labels=schema_labels)

    if config.val_csv and config.val_csv.exists():
        val_texts, val_y_full, _ = load_registry_csv(config.val_csv, required_labels=schema_labels)
    else:
        train_texts, val_texts, train_y_full, val_y_full = train_test_split(
            train_texts, train_y_full, test_size=config.val_split, random_state=42
        )

    test_texts, test_y_full, _ = load_registry_csv(config.test_csv, required_labels=schema_labels)

    active_labels, dormant_labels = _select_active_labels(schema_labels, train_y_full, config)
    if not active_labels:
        raise ValueError("No active labels selected; adjust dormant settings.")

    active_idx = [schema_labels.index(l) for l in active_labels]
    train_y = train_y_full[:, active_idx]
    val_y = val_y_full[:, active_idx]
    test_y = test_y_full[:, active_idx]

    print(f"Active labels: {len(active_labels)} | Dormant: {len(dormant_labels)}")

    # pos_weight
    pos_counts = train_y.sum(axis=0)
    neg_counts = len(train_y) - pos_counts
    pos_counts = np.maximum(pos_counts, 1)
    weights = neg_counts / pos_counts
    weights = np.minimum(weights, config.pos_weight_cap)
    pos_weight = torch.tensor(weights, dtype=torch.float32).to(config.device)

    tokenizer = HeadTailTokenizer(
        AutoTokenizer.from_pretrained(config.model_name),
        max_length=config.max_length,
        head_tokens=config.head_tokens,
        tail_tokens=config.tail_tokens,
    )

    train_ds = RegistryDataset(train_texts, train_y, tokenizer)
    val_ds = RegistryDataset(val_texts, val_y, tokenizer)
    test_ds = RegistryDataset(test_texts, test_y, tokenizer)

    train_loader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=config.batch_size)
    test_loader = DataLoader(test_ds, batch_size=config.batch_size)

    model = RoBERTaPM3MultiLabel(config.model_name, len(active_labels), pos_weight=pos_weight)
    model.to(config.device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)
    total_steps = max(1, len(train_loader) * config.num_epochs)
    warmup_steps = int(total_steps * config.warmup_ratio)
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps
    )
    scaler = torch.amp.GradScaler("cuda", enabled=config.fp16)

    best_f1 = -1.0
    thresholds = _thresholds_05(active_labels)

    for epoch in range(config.num_epochs):
        model.train()
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
            batch = {k: v.to(config.device) for k, v in batch.items()}
            with torch.amp.autocast("cuda", enabled=config.fp16):
                outputs = model(**batch)
                loss = outputs["loss"]
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()
            optimizer.zero_grad()

        # Validation
        model.eval()
        val_probs = []
        with torch.no_grad():
            for batch in val_loader:
                batch = {k: v.to(config.device) for k, v in batch.items()}
                out = model(**batch)
                val_probs.append(torch.sigmoid(out["logits"]).cpu().numpy())
        val_probs = np.vstack(val_probs) if val_probs else np.zeros((0, len(active_labels)), dtype=float)

        val_metrics = _metrics_at_thresholds(val_y, val_probs, active_labels, thresholds)
        macro_f1 = float(val_metrics["macro_f1"])
        print(f"Epoch {epoch+1} Val Macro F1: {macro_f1:.4f}")

        if macro_f1 > best_f1:
            best_f1 = macro_f1
            print("Saving best model...")
            model.save_pretrained(config.output_dir)
            tokenizer.tokenizer.save_pretrained(config.output_dir / "tokenizer")

            (config.output_dir / "label_fields.json").write_text(json.dumps(active_labels, indent=2) + "\n")
            (config.output_dir / "dormant_labels.json").write_text(json.dumps(dormant_labels, indent=2) + "\n")
            (config.output_dir / "thresholds.json").write_text(json.dumps(thresholds, indent=2) + "\n")

            metrics = {
                "best_val_macro_f1": best_f1,
                "val_metrics": val_metrics,
                "config": {
                    "model_name": config.model_name,
                    "batch_size": config.batch_size,
                    "learning_rate": config.learning_rate,
                    "num_epochs": config.num_epochs,
                    "loss": "bce",
                    "schema_labels": schema_labels,
                    "active_labels": active_labels,
                    "dormant_labels": dormant_labels,
                    "dormant_mode": config.dormant_mode,
                    "dormant_min_positives": int(config.dormant_min_positives),
                },
            }
            (config.output_dir / "metrics.json").write_text(json.dumps(metrics, indent=2) + "\n")

    # Test evaluation (on best checkpoint)
    best_model = RoBERTaPM3MultiLabel.from_pretrained(config.output_dir, num_labels=len(active_labels))
    best_model.to(config.device)
    best_model.eval()
    test_probs = []
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Testing"):
            batch = {k: v.to(config.device) for k, v in batch.items()}
            out = best_model(**batch)
            test_probs.append(torch.sigmoid(out["logits"]).cpu().numpy())
    test_probs = np.vstack(test_probs) if test_probs else np.zeros((0, len(active_labels)), dtype=float)
    test_metrics = _metrics_at_thresholds(test_y, test_probs, active_labels, thresholds)

    metrics_path = config.output_dir / "metrics.json"
    metrics = json.loads(metrics_path.read_text()) if metrics_path.exists() else {}
    metrics["test_metrics"] = test_metrics
    metrics_path.write_text(json.dumps(metrics, indent=2) + "\n")

    print(f"Training complete. Best Val Macro F1: {best_f1:.4f}")
    return metrics


def evaluate_only(model_dir: Path, config: TrainingConfig) -> dict[str, Any]:
    label_fields_path = model_dir / "label_fields.json"
    if not label_fields_path.exists():
        raise SystemExit(f"Missing label_fields.json in model dir: {model_dir}")
    label_fields = load_label_fields_json(label_fields_path)

    thresholds_path = model_dir / "thresholds.json"
    thresholds = _thresholds_05(label_fields)
    if thresholds_path.exists():
        data = json.loads(thresholds_path.read_text())
        if isinstance(data, dict):
            thresholds = {k: float(v["threshold"] if isinstance(v, dict) and "threshold" in v else v) for k, v in data.items()}

    test_texts, test_y, _ = load_registry_csv(config.test_csv, required_labels=label_fields)

    tokenizer_dir = model_dir / "tokenizer"
    tokenizer = HeadTailTokenizer(
        AutoTokenizer.from_pretrained(str(tokenizer_dir if tokenizer_dir.exists() else model_dir)),
        max_length=config.max_length,
        head_tokens=config.head_tokens,
        tail_tokens=config.tail_tokens,
    )
    test_ds = RegistryDataset(test_texts, test_y, tokenizer)
    test_loader = DataLoader(test_ds, batch_size=config.batch_size)

    model = RoBERTaPM3MultiLabel.from_pretrained(model_dir, num_labels=len(label_fields))
    model.to(config.device)
    model.eval()

    probs = []
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Evaluating"):
            batch = {k: v.to(config.device) for k, v in batch.items()}
            out = model(**batch)
            probs.append(torch.sigmoid(out["logits"]).cpu().numpy())
    probs = np.vstack(probs) if probs else np.zeros((0, len(label_fields)), dtype=float)

    metrics = _metrics_at_thresholds(test_y, probs, label_fields, thresholds)
    out = {"test_metrics": metrics, "model_dir": str(model_dir), "labels": label_fields}
    print(json.dumps(out["test_metrics"], indent=2))
    return out


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("--model-name", type=str, default=TrainingConfig.model_name)
    parser.add_argument("--output-dir", type=Path, default=Path("data/models/roberta_pm3_registry"))
    parser.add_argument("--train-csv", type=Path, default=Path("data/ml_training/registry_train.csv"))
    parser.add_argument("--val-csv", type=Path, default=Path("data/ml_training/registry_val.csv"))
    parser.add_argument("--test-csv", type=Path, default=Path("data/ml_training/registry_test.csv"))
    parser.add_argument("--batch-size", type=int, default=16)
    parser.add_argument("--epochs", type=int, default=5)
    parser.add_argument("--lr", type=float, default=2e-5)
    parser.add_argument("--no-fp16", action="store_true")
    parser.add_argument("--dormant-mode", choices=["auto", "schema", "none"], default="auto")
    parser.add_argument("--dormant-min-positives", type=int, default=20)
    parser.add_argument("--evaluate-only", action="store_true")
    parser.add_argument("--model-dir", type=Path, help="Model directory for --evaluate-only")
    args = parser.parse_args()

    config = TrainingConfig(
        model_name=args.model_name,
        train_csv=args.train_csv,
        val_csv=args.val_csv,
        test_csv=args.test_csv,
        output_dir=args.output_dir,
        batch_size=int(args.batch_size),
        num_epochs=int(args.epochs),
        learning_rate=float(args.lr),
        fp16=not bool(args.no_fp16),
        dormant_mode=str(args.dormant_mode),
        dormant_min_positives=int(args.dormant_min_positives),
    )

    if args.evaluate_only:
        if not args.model_dir:
            raise SystemExit("--model-dir is required with --evaluate-only")
        evaluate_only(Path(args.model_dir), config)
    else:
        train(config)
