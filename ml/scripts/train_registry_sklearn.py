#!/usr/bin/env python3
"""Train Registry Multi-Label Classifier (Sklearn Baseline).

This script is the direct replacement for the old `train_cpt_custom.py`.
It trains the "Registry First" baseline model (TF-IDF + Calibrated Logistic Regression)
using the cleaned training data generated by `data_prep.py`.

The trained model is used for:
1. "Scenario C" Hybrid Audit (catching procedures missed by CPT).
2. Baseline comparison for the RoBERTa model.

Pipeline:
    Input:  data/ml_training/registry_train.csv
    Output: data/models/registry_classifier.pkl
            data/models/registry_mlb.pkl
            data/models/registry_thresholds.json
"""

import argparse
import sys
from pathlib import Path

# Ensure project root is in python path
PROJECT_ROOT = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(PROJECT_ROOT))

from ml.lib.ml_coder.registry_training import train_and_evaluate, MODELS_DIR, TRAIN_CSV_PATH, TEST_CSV_PATH

def main():
    parser = argparse.ArgumentParser(description="Train Registry Sklearn Baseline Model")
    parser.add_argument(
        "--train-csv",
        type=Path,
        default=TRAIN_CSV_PATH,
        help=f"Path to training CSV (default: {TRAIN_CSV_PATH})"
    )
    parser.add_argument(
        "--test-csv",
        type=Path,
        default=TEST_CSV_PATH,
        help=f"Path to test CSV (default: {TEST_CSV_PATH})"
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=MODELS_DIR,
        help=f"Directory to save models (default: {MODELS_DIR})"
    )
    parser.add_argument(
        "--skip-eval",
        action="store_true",
        help="Skip evaluation after training"
    )

    args = parser.parse_args()

    print(f"\n{'='*60}")
    print("  Registry Model Training (Sklearn Baseline)")
    print(f"{'='*60}")
    print(f"Train Data:  {args.train_csv}")
    print(f"Test Data:   {args.test_csv}")
    print(f"Output Dir:  {args.output_dir}")
    print(f"{'='*60}\n")

    if not args.train_csv.exists():
        print(f"Error: Training file not found: {args.train_csv}")
        print("Run `python modules/ml_coder/data_prep.py` first.")
        sys.exit(1)

    try:
        if args.skip_eval:
            # Just train
            from ml.lib.ml_coder.registry_training import train_registry_model
            train_registry_model(
                train_csv=args.train_csv,
                models_dir=args.output_dir
            )
            print("\nTraining successful. Evaluation skipped.")
        else:
            # Train and Evaluate
            metrics = train_and_evaluate(
                train_csv=args.train_csv,
                test_csv=args.test_csv,
                models_dir=args.output_dir
            )
            
            print(f"\n{'='*60}")
            print("  Final Results")
            print(f"{'='*60}")
            print(f"Macro F1: {metrics['macro']['f1']:.4f}")
            print(f"Micro F1: {metrics['micro']['f1']:.4f}")
            print(f"Samples:  {metrics['n_samples']}")
            print("\nModel artifacts saved to output directory.")

    except Exception as e:
        print(f"\nCRITICAL ERROR during training: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()