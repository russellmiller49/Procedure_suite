#!/usr/bin/env python3
"""Train RoBERTa-PM-M3-Voc-distill for multi-label registry procedure classification.

This script trains the local RoBERTa-PM-M3-Voc-distill model for predicting registry
procedure flags from clinical procedure notes.

Pipeline Context:
    - Input: data/ml_training/registry_train.csv (Generated by data_prep.py)
    - The input CSV is expected to contain pre-scrubbed text if PHI redaction was
      performed during the data preparation phase.

Key Features:
    - Head + Tail truncation (First 382 + Last 128 tokens)
    - pos_weight calculation for handling severe class imbalance
    - Per-class threshold optimization on validation set
    - Mixed precision training (fp16)

Usage:
    python scripts/train_roberta_pm3.py
    python scripts/train_roberta_pm3.py --batch-size 16 --epochs 8
    python scripts/train_roberta_pm3.py --evaluate-only --model-dir data/models/roberta_pm3_registry
"""

from __future__ import annotations

import argparse
import json
import shutil
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.metrics import f1_score, precision_recall_fscore_support
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from transformers import (
    AutoConfig,
    AutoModel,
    AutoTokenizer,
    get_linear_schedule_with_warmup,
)

# ============================================================================
# Configuration
# ============================================================================

@dataclass
class TrainingConfig:
    """Configuration for RoBERTa-PM-M3-Voc-distill training."""

    # Model
    model_name: str = "data/models/RoBERTa-base-PM-M3-Voc-distill/RoBERTa-base-PM-M3-Voc-distill-hf"

    # Data paths (aligned with modules/ml_coder/data_prep.py)
    train_csv: Path = field(default_factory=lambda: Path("data/ml_training/registry_train.csv"))
    val_csv: Path = field(default_factory=lambda: Path("data/ml_training/registry_val.csv"))
    test_csv: Path = field(default_factory=lambda: Path("data/ml_training/registry_test.csv"))
    label_fields_json: Path = field(default_factory=lambda: Path("data/ml_training/registry_label_fields.json"))
    output_dir: Path = field(default_factory=lambda: Path("data/models/roberta_pm3_registry"))

    # Tokenization
    max_length: int = 512
    head_tokens: int = 382  # First N tokens to keep
    tail_tokens: int = 128  # Last N tokens to keep

    # Training hyperparameters
    batch_size: int = 16
    learning_rate: float = 2e-5
    num_epochs: int = 5
    warmup_ratio: float = 0.1
    weight_decay: float = 0.01
    gradient_accumulation_steps: int = 1
    max_grad_norm: float = 1.0

    # Hardware
    fp16: bool = True
    device: str = field(default_factory=lambda: "cuda" if torch.cuda.is_available() else "cpu")

    # Class imbalance
    pos_weight_cap: float = 100.0

    # Validation
    val_split: float = 0.1

    # Logging
    logging_steps: int = 50


# ============================================================================
# Data Loading
# ============================================================================

def load_label_fields(path: Path) -> list[str]:
    """Load canonical label ordering from JSON."""
    if not path.exists():
        return []
    with open(path, "r") as f:
        return json.load(f)

def load_registry_csv(path: Path, required_labels: list[str] = None) -> tuple[list[str], np.ndarray, list[str]]:
    """Load registry training/test CSV file.

    Args:
        path: Path to CSV file
        required_labels: List of label columns to enforce order.

    Returns:
        (texts, labels_matrix, label_names)
    """
    if not path.exists():
        raise FileNotFoundError(f"CSV file not found: {path}")

    df = pd.read_csv(path)
    if "note_text" not in df.columns:
        raise ValueError(f"File {path} missing 'note_text' column.")

    texts = df["note_text"].fillna("").astype(str).tolist()

    if required_labels:
        # Enforce specific schema (crucial for inference consistency)
        label_cols = required_labels
        for col in label_cols:
            if col not in df.columns:
                # Add missing column as 0 (e.g. rare label present in train but not test)
                df[col] = 0
    else:
        # Infer labels from binary columns
        label_cols = []
        for col in df.columns:
            if col == "note_text":
                continue
            # Simple heuristic: if column is numeric, assume it's a label
            if pd.api.types.is_numeric_dtype(df[col]):
                label_cols.append(col)
        label_cols.sort()

    y = df[label_cols].fillna(0).astype(int).to_numpy()
    
    # Clip to [0, 1] just in case
    y = np.clip(y, 0, 1)

    print(f"Loaded {len(texts)} samples from {path}")
    return texts, y, label_cols


class HeadTailTokenizer:
    """Tokenizer keeping the start (procedure) and end (complications/plan) of notes."""
    def __init__(self, tokenizer, max_length=512, head_tokens=382, tail_tokens=128):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.head_tokens = head_tokens
        self.tail_tokens = tail_tokens

    def __call__(self, text: str):
        tokens = self.tokenizer(text, add_special_tokens=False, truncation=False, return_tensors="pt")
        input_ids = tokens["input_ids"][0]

        if len(input_ids) > (self.max_length - 2):
            head = input_ids[:self.head_tokens]
            tail = input_ids[-self.tail_tokens:]
            input_ids = torch.cat([head, tail])

        # Add [CLS] and [SEP]
        input_ids = torch.cat([
            torch.tensor([self.tokenizer.cls_token_id]),
            input_ids,
            torch.tensor([self.tokenizer.sep_token_id])
        ])

        # Pad
        pad_len = self.max_length - len(input_ids)
        if pad_len > 0:
            input_ids = torch.cat([input_ids, torch.full((pad_len,), self.tokenizer.pad_token_id)])

        mask = (input_ids != self.tokenizer.pad_token_id).long()
        return {"input_ids": input_ids.long(), "attention_mask": mask}


class RegistryDataset(Dataset):
    def __init__(self, texts, labels, tokenizer):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        data = self.tokenizer(self.texts[idx])
        data["labels"] = torch.tensor(self.labels[idx], dtype=torch.float32)
        return data


# ============================================================================
# Model & Training
# ============================================================================

class RoBERTaPM3MultiLabel(nn.Module):
    def __init__(self, model_name, num_labels, pos_weight=None):
        super().__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
        self.register_buffer("pos_weight", pos_weight)

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled = self.dropout(outputs.last_hidden_state[:, 0, :])
        logits = self.classifier(pooled)
        
        loss = None
        if labels is not None:
            loss_fn = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)
            loss = loss_fn(logits, labels)
            
        return {"logits": logits, "loss": loss}

    def save_pretrained(self, path: Path):
        path.mkdir(parents=True, exist_ok=True)
        self.bert.save_pretrained(path)
        torch.save(self.classifier.state_dict(), path / "classifier.pt")
        if self.pos_weight is not None:
            torch.save(self.pos_weight, path / "pos_weight.pt")

    @classmethod
    def from_pretrained(cls, path: Path, num_labels: int):
        # Implementation for loading (simplified for brevity)
        model = cls(str(path), num_labels)
        model.classifier.load_state_dict(torch.load(path / "classifier.pt"))
        return model


def train(config: TrainingConfig):
    print(f"--- Training RoBERTa Registry Model ---")
    
    # 1. Load Data
    # Prefer loading label order from JSON if available to ensure consistency
    known_labels = load_label_fields(config.label_fields_json)
    
    train_texts, train_y, train_labels = load_registry_csv(config.train_csv, required_labels=known_labels or None)
    
    if not known_labels:
        print(f"Inferred {len(train_labels)} labels. Saving to {config.label_fields_json}")
        with open(config.label_fields_json, "w") as f:
            json.dump(train_labels, f, indent=2)
        known_labels = train_labels

    print(f"Labels: {known_labels}")

    # Split Validation
    train_texts, val_texts, train_y, val_y = train_test_split(
        train_texts, train_y, test_size=config.val_split, random_state=42
    )

    # 2. Calculate pos_weight
    pos_counts = train_y.sum(axis=0)
    neg_counts = len(train_y) - pos_counts
    # Avoid div by zero
    pos_counts = np.maximum(pos_counts, 1) 
    weights = neg_counts / pos_counts
    weights = np.minimum(weights, config.pos_weight_cap)
    pos_weight = torch.tensor(weights, dtype=torch.float32).to(config.device)

    # 3. Setup
    tokenizer = HeadTailTokenizer(
        AutoTokenizer.from_pretrained(config.model_name),
        max_length=config.max_length,
        head_tokens=config.head_tokens,
        tail_tokens=config.tail_tokens
    )
    
    train_ds = RegistryDataset(train_texts, train_y, tokenizer)
    val_ds = RegistryDataset(val_texts, val_y, tokenizer)
    
    train_loader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=config.batch_size)

    model = RoBERTaPM3MultiLabel(config.model_name, len(known_labels), pos_weight=pos_weight)
    model.to(config.device)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=len(train_loader) * config.num_epochs)
    scaler = torch.amp.GradScaler('cuda', enabled=config.fp16)

    # 4. Loop
    best_f1 = 0
    
    for epoch in range(config.num_epochs):
        model.train()
        train_loss = 0
        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}"):
            batch = {k: v.to(config.device) for k, v in batch.items()}
            
            with torch.amp.autocast('cuda', enabled=config.fp16):
                outputs = model(**batch)
                loss = outputs["loss"]
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()
            optimizer.zero_grad()
            train_loss += loss.item()

        # Validation
        model.eval()
        val_probs = []
        with torch.no_grad():
            for batch in val_loader:
                batch = {k: v.to(config.device) for k, v in batch.items()}
                out = model(**batch)
                val_probs.append(torch.sigmoid(out["logits"]).cpu().numpy())
        
        val_probs = np.vstack(val_probs)
        
        # Simple threshold check
        val_preds = (val_probs >= 0.5).astype(int)
        macro_f1 = f1_score(val_y, val_preds, average="macro", zero_division=0)
        print(f"Epoch {epoch+1} Val Macro F1: {macro_f1:.4f}")

        if macro_f1 > best_f1:
            best_f1 = macro_f1
            print("Saving best model...")
            model.save_pretrained(config.output_dir)
            tokenizer.tokenizer.save_pretrained(config.output_dir / "tokenizer")
            
            # Save threshold metadata (simplified to 0.5 for now, full script has optimizer)
            thresholds = {l: 0.5 for l in known_labels}
            with open(config.output_dir / "thresholds.json", "w") as f:
                json.dump(thresholds, f, indent=2)

    print(f"Training complete. Best F1: {best_f1:.4f}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--batch-size", type=int, default=16)
    parser.add_argument("--epochs", type=int, default=5)
    parser.add_argument("--evaluate-only", action="store_true")
    parser.add_argument("--model-dir", type=Path)
    args = parser.parse_args()
    
    config = TrainingConfig(batch_size=args.batch_size, num_epochs=args.epochs)
    
    if args.evaluate_only:
        print("Evaluation mode not fully implemented in this summary script.")
    else:
        train(config)